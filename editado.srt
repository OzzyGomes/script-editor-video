1
00:00:05,239 --> 00:00:05,479
Thank you.

2
00:00:12,719 --> 00:00:13,119
Okay.

3
00:00:22,960 --> 00:00:27,280
Salve galera ligada no canal Beleza dos Dados, tudo bem com vocês?

4
00:00:27,760 --> 00:00:30,080
Como é que vocês estão? Espero que todos estejam bem.

5
00:00:30,840 --> 00:00:34,540
Bem-vindo a mais uma aula de Databricks e Piespark aqui no canal.

6
00:00:35,300 --> 00:00:42,240
Hoje é uma aula muito especial, nós daremos continuidade às features presentes.

7
00:00:43,380 --> 00:00:56,340
Nós daremos continuidade hoje as features presente nas tabelas Delta e a gente não pode deixar de falar de citar a respeito da feature evolução de esquema, ou...

8
00:00:56,609 --> 00:00:58,969
esquema-evolution.

9
00:01:00,600 --> 00:01:01,780
Eeeeeeee

10
00:01:12,030 --> 00:01:17,510
Nessa aula, teremos uma leve introdução teórica ao que é o esquema evolution

11
00:01:17,510 --> 00:01:21,569
e quando você precisa usar qual a diferença que o Delta aplica

12
00:01:21,569 --> 00:01:25,409
em relação aos outros Data Warehouses.

13
00:01:26,290 --> 00:01:30,829
E depois disso, teremos uma rápida demonstração

14
00:01:30,829 --> 00:01:34,409
com aquela mesma tabela que nós criamos nos vídeos anteriores.

15
00:01:36,329 --> 00:01:40,149
Para quem assistiu o último vídeo, falamos sobre...

16
00:01:41,099 --> 00:01:47,719
o Time Travel, que é uma das vantagens mais interessantes do Delta Table.

17
00:02:05,579 --> 00:02:08,659
Na aula passada, nós falamos sobre Time Travel,

18
00:02:09,120 --> 00:02:13,500
que é um dos conceitos do Delta que eu mais gosto.

19
00:02:13,759 --> 00:02:15,840
Se você não assistiu, assista.

20
00:02:15,900 --> 00:02:19,819
Vai ser de grande valia para sua jornada com dados.

21
00:02:20,199 --> 00:02:23,840
E hoje nós iremos falar sobre esquema evoluixo.

22
00:02:24,000 --> 00:02:24,439
Vamos para a aula.

23
00:02:29,219 --> 00:02:29,639
boom

24
00:02:36,650 --> 00:02:38,969
Evolução de esquema em tabelas Delta.

25
00:02:39,490 --> 00:02:43,669
Adaptando suas tabelas de dados de forma segura e eficiente.

26
00:02:47,400 --> 00:02:52,080
O problema não é comum, mudança de esquema ou esquema.

27
00:02:53,759 --> 00:02:56,360
Então, primeiramente, para a gente...

28
00:02:56,819 --> 00:03:01,000
iniciávamos deixar claro o que é o esquema, tá?

29
00:03:01,799 --> 00:03:05,620
E de maneira mais resumida possível seria a estrutura de uma tabela.

30
00:03:06,180 --> 00:03:09,300
Nomes das colunas, tipos de dados.

31
00:03:10,109 --> 00:03:11,809
e se podem ser ou não nulos.

32
00:03:13,529 --> 00:03:13,589
Thank you.

33
00:03:15,000 --> 00:03:16,599
Qual é o desafio tradicional?

34
00:03:19,289 --> 00:03:27,729
Em data warehouses, em data lakes tradicionais, alterar a esquema de uma tabela com dados existentes

35
00:03:27,729 --> 00:03:35,049
é uma operação complexa e arriscada, ou seja, entendendo que o esquema pode ser...

36
00:03:35,460 --> 00:03:39,520
Essa estrutura da tabela, nome de coluna, tipo de dados e afins.

37
00:03:41,599 --> 00:03:53,240
Frequentemente, o que é que isso exige? Se você está em um data house tradicional, você tem que dar um alter table no SQL, isso é bastante complexo e bastante arriscado.

38
00:03:54,239 --> 00:03:58,639
Recreação completa da tabela e recarregar os dados é uma outra opção também,

39
00:03:59,019 --> 00:04:03,659
que eu não recomendo, o seu apetitor do histórico vai ter muito mais riscos,

40
00:04:04,479 --> 00:04:09,199
e falando de risco, riscos de falhas e corrupção de dados no meio do processo.

41
00:04:10,409 --> 00:04:12,349
Beleza!

42
00:04:13,409 --> 00:04:16,370
e a gente fala sobre o problema comum

43
00:04:17,370 --> 00:04:18,670
na mudança de esquema.

44
00:04:25,770 --> 00:04:26,950
Então...

45
00:04:30,960 --> 00:04:32,340
ʔ ʔ ʔ ʔ ʔ

46
00:04:33,300 --> 00:04:35,139
O que é o esquema Evolution?

47
00:04:36,990 --> 00:04:39,810
já que ele é diferente do tradicional.

48
00:04:51,480 --> 00:04:58,300
É a capacidade do Delta Lake de permitir que os usuários atualizem um esquema de uma tabela

49
00:04:58,300 --> 00:04:59,780
de forma fácil e segura.

50
00:05:00,300 --> 00:05:00,340
Thank you.

51
00:05:02,160 --> 00:05:09,740
O mais comum é adicionar novas colunas a uma tabela à medida que os dados de origem mudam sem a necessidade.

52
00:05:10,320 --> 00:05:12,240
de reescrever os dados existentes.

53
00:05:13,770 --> 00:05:21,010
E aí, como funciona? O delta-lay quer armazenar o esquema da tabela como parte de metadados

54
00:05:21,010 --> 00:05:27,130
no log de transações. Isso permite que ele valide e adapte o esquema durante a operação

55
00:05:31,379 --> 00:05:37,120
Vamos lá, é, tem duas coisas interessantes aqui, né, quando se fala o que é o Skim Evolution.

56
00:05:37,920 --> 00:05:46,840
Basicamente, ele acontece no processo de escrita, imagine que você tem um pipeline que ele está sempre sendo atualizado e...

57
00:05:47,220 --> 00:05:50,080
normalmente o paquilana ele já espera

58
00:05:50,939 --> 00:05:57,620
um determinado conjunto de colunas de dados, de tipos de dados dessa coluna, dessa tabela

59
00:06:06,330 --> 00:06:08,930
E como ele já espera esses tipos de dados?

60
00:06:10,379 --> 00:06:11,819
Uma vez que...

61
00:06:12,689 --> 00:06:21,230
estes dados não são validados, se estes dados não forem iguais a isso, que seria a forma mais comum.

62
00:06:28,470 --> 00:06:28,970
Thank you.

63
00:06:37,190 --> 00:06:45,130
que seria a forma mais comum de adicionar dados ou a imposição de esquema, ou seja, o delta ele falar não.

64
00:06:45,720 --> 00:07:02,980
Eu espero que tenham 10 colunas, essas 10 colunas, 5 delas, e aqui é um exemplo, podem acertar dados nulos ou não, e cada coluna tem, temos string, temos floats, temos double, temos...

65
00:07:04,590 --> 00:07:08,370
Ou seja, ele já tem uma configuração e ele vai impor esse esquema.

66
00:07:09,120 --> 00:07:09,579
Polin!

67
00:07:10,410 --> 00:07:16,350
nem sempre nós iremos, nem sempre se...

68
00:07:16,890 --> 00:07:23,070
Porém, nem sempre isso vai ser legal para o seu pipeline, às vezes você lida com dados

69
00:07:23,070 --> 00:07:27,490
externos e você precisa que esses dados sejam atualizados.

70
00:07:28,439 --> 00:07:36,240
com os dados da tabela. Imagina que essa tabela vem de outro local, que não o Databricks.

71
00:07:37,060 --> 00:07:41,360
E, uma vez que ela chega no Databricks, você pega esses dados...

72
00:07:42,689 --> 00:07:44,870
Processa encamadas bronze-silver-gold.

73
00:07:47,850 --> 00:07:50,730
Então, se você não tem...

74
00:07:51,300 --> 00:07:56,960
Se você não domina o arquivo como por inteiro, você não consegue garantir que sempre terá dez colunas.

75
00:07:57,480 --> 00:08:02,379
Voltando para as nossas amplias de dez colunas, ou seja, se o nome da coluna mudar...

76
00:08:02,910 --> 00:08:11,630
Se outra coluna for adicionado, se o tipo de dado for mudado, se o tipo de informação

77
00:08:11,630 --> 00:08:14,730
que essa coluna recebe, se pode ou não tem nulo, for mudado.

78
00:08:15,689 --> 00:08:20,709
nós teremos o meio no nosso pipeline, o que vai acarretar e não chegar dados

79
00:08:21,170 --> 00:08:23,610
para as nossas tabelas, e não é isso que a gente quer.

80
00:08:23,889 --> 00:08:27,889
E é isso que o Delta Scheme Evolution faz.

81
00:08:28,969 --> 00:08:30,389
Ele nos garante...

82
00:08:30,930 --> 00:08:34,950
a possibilidade de não termos erros no Deutaframe.

83
00:08:35,070 --> 00:08:39,649
Uma vez que o Scheme Evolution é ligado, ele vai adaptar...

84
00:08:40,350 --> 00:08:44,409
a sua tabela delta para noar os novos tipos de dados que estão chegando.

85
00:08:47,629 --> 00:08:50,829
Continuando e aí como é que essa mágica acontece?

86
00:08:51,289 --> 00:08:55,049
Essa mágica acontece com um parâmetro chamado merge schema.

87
00:08:56,490 --> 00:09:02,289
que é a opção-chave, Options, Merge Schema True.

88
00:09:03,090 --> 00:09:07,690
Por padrão, esse mode de esquema sempre vai estar acetado como false.

89
00:09:08,530 --> 00:09:14,870
E aí você só vai colocar essa opção no momento da escrita como verdadeiro. True.

90
00:09:16,409 --> 00:09:22,149
A esta opção que você adiciona ao seu comando de escrita write, ou write stream no passpark.

91
00:09:30,430 --> 00:09:34,790
que está sendo escrito como uma esquema da tabela dentro do...

92
00:09:36,420 --> 00:09:37,940
ao ser definida como Choo.

93
00:09:38,580 --> 00:09:42,700
Ela instrui o Delta Lake a comparar o esquema do DataFrame.

94
00:09:43,290 --> 00:09:46,750
que está sendo escrito com o esquema da tabela delta de destino.

95
00:09:55,649 --> 00:09:56,389
Thank you.

96
00:09:57,600 --> 00:10:01,700
Então, o que acontece se usar o merge schema?

97
00:10:04,170 --> 00:10:06,630
o que acontece ao usar uma esquema.

98
00:10:08,220 --> 00:10:12,100
novas colunas. Então, se essa nova tabela...

99
00:10:13,050 --> 00:10:16,130
que pode chegar todo dia. Imaginem, todo dia chega um arquivo CSV.

100
00:10:18,149 --> 00:10:27,689
Vamos fazer um exemplo bem bacana para a gente entender ainda mais o que é o mais de esquema, imagina que todo dia chega um arquivo CSV.

101
00:10:29,100 --> 00:10:30,759
e de uma fábrica.

102
00:10:31,500 --> 00:10:37,240
Você está pegando esses arquivos e compilando em uma super tabela Delta em um determinado dia.

103
00:10:38,879 --> 00:10:40,840
Esse arquivo recebe novas colundas.

104
00:10:41,970 --> 00:10:47,230
E aí o que acontece é, se o datafermo de origem tiver colunas que não existem na tabela de destino,

105
00:10:47,790 --> 00:10:52,730
essas serão adicionadas ao esquema da tabela, simplesmente assim. Então, se eu tinha 10 colunas.

106
00:10:53,730 --> 00:10:57,909
Os arquivos agora vem com 12 colunas, essas colunas automaticamente serão adicionadas.

107
00:10:58,500 --> 00:10:59,799
dentro desse arquivo.

108
00:11:00,750 --> 00:11:03,629
Ah, Ozzy, o que acontece com os dias anteriores então?

109
00:11:04,370 --> 00:11:06,350
Esses dias anteriores vão estar todos como nulos.

110
00:11:07,800 --> 00:11:13,700
valores nulos. As linhas existentes antigas, exatamente o que eu acabei de dizer, não terão valores.

111
00:11:26,340 --> 00:11:31,480
As linhas existentes antigas não terão valores para as novas colunas.

112
00:11:32,259 --> 00:11:36,080
Ao serem lidas, o Spark preencherá essas colunas com NULL.

113
00:11:38,100 --> 00:11:38,620
Thank you.

114
00:11:39,570 --> 00:11:45,450
tipo de dados. A evolução de esquema também pode, de forma segura, ampliar tipo de dados.

115
00:11:46,290 --> 00:11:54,350
Integer para long, mas não permite mudanças que causem troncamento, string para integer.

116
00:12:07,850 --> 00:12:14,990
Então você pode atualizar de acordo com o tipo de dados do Spark, então se ela entender se o

117
00:12:15,540 --> 00:12:18,760
Ah, o esquema evolúcio, entender.

118
00:12:19,470 --> 00:12:24,769
que é mais interessante fazer essa migração de íntese para a longa ela vai fazer mas nunca vai

119
00:12:24,769 --> 00:12:28,490
fazer uma mudança que causa e erro no

120
00:12:29,909 --> 00:12:34,230
no Spark que seria mudar de string para integer, tá?

121
00:12:38,579 --> 00:12:38,919
Та-а-а-а!

122
00:12:39,330 --> 00:12:41,270
para finalizar quando usar.

123
00:12:42,990 --> 00:12:47,730
E aí a gente tem a opção por padrão, que é a imposição de esquema.

124
00:12:48,950 --> 00:12:55,750
Na maioria das operações de rotina ETL diário, você pode usar sim a imposição de esquema.

125
00:12:56,610 --> 00:12:56,649
Thank you.

126
00:12:57,540 --> 00:13:02,500
Quando você espera que os dados de entrada correspondam exatamente ao esquema da tabela.

127
00:13:03,060 --> 00:13:08,440
Tá? Então no mundo ideal. Principalmente quando os dados vêm de bancos de dados.

128
00:13:09,360 --> 00:13:13,300
que raramente vai ser alterado os dados quando vem de bancos de dados.

129
00:13:14,370 --> 00:13:21,090
Agora, quando vem de CSVs, quando vem de APIs, é muito mais fácil ter essa mudança.

130
00:13:26,310 --> 00:13:29,270
para garantir a integridade de consistência dos dados.

131
00:13:30,600 --> 00:13:33,399
Use a evolução de esquema.

132
00:13:34,350 --> 00:13:42,029
E aí, novamente eu frisando Mary Schema e Call True quando você precisar adicionar novas colunas na sua tabela.

133
00:13:42,899 --> 00:13:48,399
novas features em modelos de machine learning, novos campos de log, tá?

134
00:13:49,000 --> 00:13:53,899
e principalmente, eu recebo aqui o CSVs, eu recebo aqui o arquivo de uma fonte

135
00:13:53,899 --> 00:13:58,000
que eu não tenho certeza que essa fonte vai se manter as mesmas, por exemplo,

136
00:13:58,740 --> 00:14:03,919
no mundo de engenharia de dados, você também pode, a qualquer momento, estar trabalhando com XS

137
00:14:04,860 --> 00:14:09,700
e o Excel por feito por humano e humano pode em algum momento falar poxa

138
00:14:09,700 --> 00:14:18,940
tem uma coluna chamada valores eu poderia colocar para preços que eu acho que fica melhor para identificar essa coluna

139
00:14:18,940 --> 00:14:20,240
e aí você vai perder

140
00:14:21,779 --> 00:14:27,699
todo o seu pipeline vai quebrar porque dessa mudança que o usuário fez então aí sim faz

141
00:14:28,920 --> 00:14:32,020
total sentido a esquema evoluja.

142
00:14:32,940 --> 00:14:38,220
Quando a fonte de dados evolui, você quer que a sua tabela Delta se adapte automaticamente.

143
00:14:40,720 --> 00:14:44,380
Durante a ingestão de dados de fontes que podem mudar com o tempo.

144
00:14:45,269 --> 00:14:51,409
Então, exatamente é isso. Eu aconselho a vocês que conseguiram chegar nesses slides aqui.

145
00:14:52,769 --> 00:15:00,029
tirar um print da tela, anotar isso em algum lugar, ter em mente o que é a composição de esquema

146
00:15:00,029 --> 00:15:06,809
que é por padrão do delta e o que é a evolução de esquema e espero que vocês tenham entendido

147
00:15:07,560 --> 00:15:12,800
como aplicar isso no seu dia a dia a teoria em si.

148
00:15:13,100 --> 00:15:20,140
Agora nós já iremos para a prática, mas o interessante é vocês entenderem de fato

149
00:15:20,140 --> 00:15:24,600
como é, como em outros bancos de dados, como é muito mais difícil, você tem que colocar

150
00:15:24,600 --> 00:15:30,460
o alter table e fazer todas as alterações, e aqui no Delta é totalmente diferente, tá?

151
00:15:31,110 --> 00:15:34,090
Beleza, é agora!

152
00:15:34,830 --> 00:15:39,509
Eu irei para o Databricks e a gente vai conversar mais a respeito.

153
00:15:47,399 --> 00:15:47,699
Thank you.

